# DataShelf - Product Data Explorer

DataShelf is a comprehensive product data exploration platform that crawls the World of Books website to extract, normalize, and serve structured product data. The system consists of a web scraper service, a PostgreSQL database (Supabase), edge API workers (Cloudflare), and a React frontend.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React Frontend â”‚    â”‚ Cloudflare Workersâ”‚    â”‚  Scraper Service â”‚
â”‚  (Cloudflare     â”‚â—„â”€â”€â–ºâ”‚     API          â”‚â—„â”€â”€â–ºâ”‚   (Render)      â”‚
â”‚   Pages)         â”‚    â”‚                  â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                        â”‚
                                â–¼                        â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Supabase      â”‚    â”‚  Cloudflare R2  â”‚
                       â”‚  PostgreSQL     â”‚    â”‚   Image Storage â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Node.js 18+ and npm
- Docker (for scraper service)
- Supabase account
- Cloudflare account (Workers, Pages, R2)
- Render account (for scraper deployment)

### 1. Clone and Setup

```bash
git clone <repository-url>
cd datashelf
npm install
```

### 2. Environment Configuration

Copy environment templates and configure:

```bash
# Root level
cp .env.example .env.local

# API (Cloudflare Workers)
cp api/.env.example api/.env

# Scraper service
cp scraper/.env.example scraper/.env

# Frontend
cp frontend/.env.example frontend/.env.local

# Database
cp database/.env.example database/.env
```

### 3. Database Setup

```bash
cd database
npm install
npm run migrate
npm run seed
```

### 4. Start Development Services

```bash
# Terminal 1: Start scraper service
cd scraper
npm install
npm run dev

# Terminal 2: Start API (Cloudflare Workers)
cd api
npm install
npm run dev

# Terminal 3: Start frontend
cd frontend
npm install
npm run dev
```

## ğŸ“ Project Structure

```
datashelf/
â”œâ”€â”€ api/                    # Cloudflare Workers API
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ handlers/       # API endpoint handlers
â”‚   â”‚   â”œâ”€â”€ utils/          # Utilities and helpers
â”‚   â”‚   â””â”€â”€ index.ts        # Main worker entry point
â”‚   â””â”€â”€ wrangler.toml       # Cloudflare Workers config
â”œâ”€â”€ database/               # Database migrations and utilities
â”‚   â”œâ”€â”€ migrations/         # SQL migration files
â”‚   â”œâ”€â”€ seeds/             # Database seed data
â”‚   â””â”€â”€ utils/             # Database utilities
â”œâ”€â”€ frontend/              # React frontend application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ pages/         # Page components
â”‚   â”‚   â”œâ”€â”€ hooks/         # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ services/      # API services
â”‚   â”‚   â””â”€â”€ utils/         # Frontend utilities
â”‚   â””â”€â”€ public/            # Static assets
â”œâ”€â”€ scraper/               # Web scraper service
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ scrapers/      # Scraping logic
â”‚   â”‚   â”œâ”€â”€ utils/         # Scraper utilities
â”‚   â”‚   â”œâ”€â”€ routes/        # HTTP endpoints
â”‚   â”‚   â””â”€â”€ worker.ts      # Main worker process
â”‚   â””â”€â”€ Dockerfile         # Docker configuration
â””â”€â”€ scripts/               # Deployment and utility scripts
```

## ğŸ”§ Component Documentation

### [Scraper Service](./scraper/README.md)
- Web scraping with Crawlee and Playwright
- PostgreSQL job queue management
- Image processing and R2 upload
- Docker deployment on Render

### [API Layer](./api/README.md)
- Cloudflare Workers edge API
- Cache-first with stale-while-revalidate
- Supabase integration
- Request authentication

### [Frontend Application](./frontend/README.md)
- React with TypeScript and Tailwind CSS
- React Query for state management
- Responsive design and accessibility
- Cloudflare Pages deployment

### [Database](./database/README.md)
- PostgreSQL schema and migrations
- Supabase configuration
- Job queue implementation
- Performance optimization

## ğŸš€ Deployment

See [DEPLOYMENT.md](./DEPLOYMENT.md) for detailed deployment instructions.

## ğŸ“Š Monitoring

- Health checks: `/health` endpoints on all services
- Logs: Structured logging with appropriate levels
- Metrics: Scraping success rates and API performance
- Alerts: Critical failure notifications

## ğŸ§ª Testing

```bash
# Run all tests
npm test

# Component-specific tests
cd scraper && npm test
cd api && npm test
cd frontend && npm test
cd database && npm test
```

## ğŸ” Troubleshooting

See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for common issues and solutions.

## ğŸ“š API Documentation

See [API.md](./API.md) for detailed API documentation with request/response examples.

## ğŸ—„ï¸ Database Schema

See [DATABASE_SCHEMA.md](./DATABASE_SCHEMA.md) for complete database documentation.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.